import re
import emoji
from soynlp.normalizer import repeat_normalize
from konlpy.tag import Mecab ## linux ì—ì„œë§Œ
from konlpy.tag import Okt
from zipf_heaps import plot_heaps, plot_zipf
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import requests

url = "https://gist.githubusercontent.com/spikeekips/40eea22ef4a89f629abd87eed535ac6a/raw/4f7a635040442a995568270ac8156448f2d1f0cb/stopwords-ko.txt"

response = requests.get(url)
if response.status_code == 200:
    stopwords = set(response.text.splitlines())
else:
    stopwords = set()
    print("ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


file_path = "merged_comments.xlsx"
df = pd.read_excel(file_path)

abs_path = os.path.abspath(file_path)
print(f"ğŸ” Trying to load file at: {abs_path}")
print(f"ğŸ“‚ File exists? {os.path.exists(abs_path)}")

## ì •ê·œí‘œí˜„ì‹ ì „ì²˜ë¦¬
pattern = re.compile('[^ ã„±-ã…£ê°€-í£]+')
url_pattern = re.compile(r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)')

def clean(x):
    x = pattern.sub(' ', x)
    x = emoji.replace_emoji(x, replace='')
    x = url_pattern.sub('', x)
    x = x.strip()
    x = repeat_normalize(x, num_repeats=2)
    return x

df_cleaned = df["a"].dropna().astype(str).apply(clean)
df_cleaned = df_cleaned[df_cleaned.str.strip() != ''].to_frame(name='a').reset_index(drop=True)

mecab = Mecab()

## ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ì¶”ì¶œ
## NN, VV
def extract_meaningful_words(text):
    pos_tags = mecab.pos(text)
    meaningful = [
        word for word, tag in pos_tags
        if tag.startswith(('NNG', 'NNP', 'VV', 'VA')) and word not in stopwords
    ]
    return meaningful

## Bigram
def generate_ngrams(words, n=2):
    return [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]

## ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ ì¶”ì¶œ ë° Bigram ìƒì„±
comments = df_cleaned['a'].tolist()
tokenized_comments = [extract_meaningful_words(comment) for comment in comments]

all_words = [word for tokens in tokenized_comments for word in tokens]
bigrams = [generate_ngrams(tokens, n=2) for tokens in tokenized_comments]
flat_bigrams = [item for sublist in bigrams for item in sublist]

## ë¹ˆë„ ê³„ì‚°
word_counts = Counter(all_words).most_common(50)
bigram_counts = Counter(flat_bigrams).most_common(50)

words, counts = zip(*word_counts)
plt.figure(figsize=(16, 6))
sns.barplot(x=list(words), y=list(counts), palette='coolwarm')
plt.xticks(rotation=45, ha='right')
plt.title('ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ ì¶œí˜„ ë¹ˆë„ Top 50', fontsize = 18)
plt.xlabel('ë‹¨ì–´', fontsize = 14)
plt.ylabel('ë¹ˆë„ìˆ˜', fontsize = 14)
plt.tight_layout()
plt.show()

bigrams, bigram_freqs = zip(*bigram_counts)
plt.figure(figsize=(16, 6))
sns.barplot(x=list(bigrams), y=list(bigram_freqs), palette='crest')
plt.xticks(rotation=45, ha='right')
plt.title('ì˜ë¯¸ ìˆëŠ” Bigram ì¶œí˜„ ë¹ˆë„ Top 50',fontsize = 18)
plt.xlabel('Bigram',fontsize = 14)
plt.ylabel('ë¹ˆë„ìˆ˜',fontsize = 14)
plt.tight_layout()
plt.show()

plot_zipf(all_words)
plot_heaps(tokenized_comments)
