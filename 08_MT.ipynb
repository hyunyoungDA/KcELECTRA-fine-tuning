{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunyoungDA/KcELECTRA-fine-tuning/blob/main/08_MT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cordless-store",
      "metadata": {
        "id": "cordless-store"
      },
      "source": [
        "# 기계 번역\n",
        "\n",
        "영-한 기계번역을 통한 인코더-디코더 모델 실습. 학습 데이터는 http://www.manythings.org/anki/kor-eng.zip 에서 다운로드.\n",
        "참고: https://wikidocs.net/86900\n",
        "\n",
        "이 학습 데이터가 적절해서라기보다는 소규모로 간단히 실습을 진행할 수 있어서 사용함."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "damaged-battle",
      "metadata": {
        "id": "damaged-battle"
      },
      "source": [
        "## 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "satellite-prevention",
      "metadata": {
        "id": "satellite-prevention"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import shutil\n",
        "import os\n",
        "import unicodedata\n",
        "import requests\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tested-membrane",
      "metadata": {
        "id": "tested-membrane"
      },
      "outputs": [],
      "source": [
        "# 결과 재현을 위한 seed값 설정\n",
        "np.random.seed(17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9857745",
      "metadata": {
        "id": "b9857745"
      },
      "outputs": [],
      "source": [
        "url ='http://www.manythings.org/anki/kor-eng.zip'\n",
        "filename = 'kor-eng.zip'\n",
        "path = os.path.join(os.getcwd(), 'rsc/kor-eng')\n",
        "text_file = os.path.join(path, 'kor.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de50324b",
      "metadata": {
        "id": "de50324b"
      },
      "outputs": [],
      "source": [
        "# 영어-한국어 병렬 코퍼스 다운로드\n",
        "# kor-eng 폴더에 압축파일을 다운로드하여 압축 해제\n",
        "# 이 코퍼스는 병렬 코퍼스를 사용한 번역 학습 과정을 샘플로 보이기 위한 것으로서,\n",
        "# 품질이 좋은 코퍼스는 아님.\n",
        "\n",
        "def download(url, output_file_name):\n",
        "    '''URL 다운로드하여 output_file_name에 저장'''\n",
        "    with open(output_file_name, 'wb') as out:\n",
        "        r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'})\n",
        "        print(r)\n",
        "        out.write(r.content)\n",
        "\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "zip_file_name = os.path.join(path, filename)\n",
        "if not os.path.exists(zip_file_name):\n",
        "    download(url, zip_file_name)\n",
        "\n",
        "if not os.path.exists(text_file):\n",
        "    with zipfile.ZipFile(zip_file_name, 'r') as zipf:\n",
        "        zipf.extractall(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "danish-terrorist",
      "metadata": {
        "id": "danish-terrorist"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# 문장 전처리 함수\n",
        "def preprocess_sentence(sent):\n",
        "    # 단어와 구두점 사이에 공백을 끼워 넣음. 예: \"he is a boy.\" -> \"he is a boy .\"\n",
        "    sent = re.sub(r\"([?.!,])\", r\" \\1\", sent)\n",
        "\n",
        "    # [단어 구성 문자들, \".\", \"?\", \"!\", \",\"]를 제외한 나머지 문자는 모두 공백으로 변환\n",
        "    sent = re.sub(r\"[^\\w!.?,]+\", r\" \", sent)\n",
        "\n",
        "    # 연속된 공백은 하나로 변환\n",
        "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "\n",
        "    return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "macro-ground",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "macro-ground",
        "outputId": "b8a0832a-4618-48dd-cd26-0cb09b3ac812"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I knew that Tom was just a freshman , so I was surprised to see him hanging out with seniors .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "preprocess_sentence(\"I knew that Tom was just a freshman, so I was surprised to see him hanging out with seniors.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "coated-rotation",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "coated-rotation",
        "outputId": "69933635-102a-404c-a154-faf2347b378e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'난 톰이 그냥 신입생일 뿐이라고만 알았는데 , 그러다보니 톰이랑 선배들이 서로 어울려다니는 걸 보고 놀랐어 .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "preprocess_sentence(\"난 톰이 그냥 신입생일 뿐이라고만 알았는데, 그러다보니 톰이랑 선배들이 서로 어울려다니는 걸 보고 놀랐어.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "leading-wales",
      "metadata": {
        "id": "leading-wales"
      },
      "outputs": [],
      "source": [
        "# 병렬 코퍼스를 전처리하여 로딩(영한 번역용)\n",
        "# 파일 내용은 영어-한글 순으로 작성되어 있음\n",
        "def load_preprocessed_data(fname):\n",
        "    encoder_input, decoder_input, decoder_target = [], [], []\n",
        "\n",
        "    with open(fname, \"r\", encoding='utf8') as f:\n",
        "        for line in f.readlines():\n",
        "            # source 데이터와 target 데이터 분리\n",
        "            src_line, tar_line, _ = line.strip().split('\\t')\n",
        "\n",
        "            # source 데이터 전처리(영어)\n",
        "            src_line_input = [w for w in preprocess_sentence(src_line).split()]\n",
        "\n",
        "            # target 데이터 전처리(한국어)\n",
        "            tar_line = preprocess_sentence(tar_line)\n",
        "            tar_line_input = [w for w in (\"<sos> \" + tar_line).split()]  # 디코더 입력용\n",
        "            tar_line_target = [w for w in (tar_line + \" <eos>\").split()]  # 출력용\n",
        "\n",
        "            encoder_input.append(src_line_input)\n",
        "            decoder_input.append(tar_line_input)\n",
        "            decoder_target.append(tar_line_target)\n",
        "\n",
        "    # (인코더 입력, 디코더 입력, 디코더 출력) 리턴\n",
        "    return encoder_input, decoder_input, decoder_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "arctic-restoration",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arctic-restoration",
        "outputId": "3372248a-2bd0-4559-a5ca-3e3367ef8eda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Go', '.'], ['Hi', '.'], ['Run', '!'], ['Run', '.'], ['Who', '?']]\n",
            "[['<sos>', '가', '.'], ['<sos>', '안녕', '.'], ['<sos>', '뛰어', '!'], ['<sos>', '뛰어', '.'], ['<sos>', '누구', '?']]\n",
            "[['가', '.', '<eos>'], ['안녕', '.', '<eos>'], ['뛰어', '!', '<eos>'], ['뛰어', '.', '<eos>'], ['누구', '?', '<eos>']]\n"
          ]
        }
      ],
      "source": [
        "sents_src_in, sents_tar_in, sents_tar_out = load_preprocessed_data(text_file)\n",
        "print(sents_src_in[:5])\n",
        "print(sents_tar_in[:5])\n",
        "print(sents_tar_out[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unable-attention",
      "metadata": {
        "id": "unable-attention"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# 정수 인코딩\n",
        "# 토큰 제거 없이 모두 사용\n",
        "tok_src = Tokenizer(filters=\"\", lower=False)\n",
        "tok_src.fit_on_texts(sents_src_in)\n",
        "encoder_input = tok_src.texts_to_sequences(sents_src_in)\n",
        "\n",
        "tok_tar = Tokenizer(filters=\"\", lower=False)\n",
        "tok_tar.fit_on_texts(sents_tar_in)\n",
        "tok_tar.fit_on_texts(sents_tar_out)\n",
        "decoder_input = tok_tar.texts_to_sequences(sents_tar_in)\n",
        "decoder_target = tok_tar.texts_to_sequences(sents_tar_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "novel-internship",
      "metadata": {
        "id": "novel-internship"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 패딩\n",
        "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
        "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
        "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prepared-luxembourg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prepared-luxembourg",
        "outputId": "da47fef3-e901-4040-ebc3-d01232336bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5890, 110)\n",
            "(5890, 96)\n",
            "(5890, 96)\n"
          ]
        }
      ],
      "source": [
        "print(encoder_input.shape)\n",
        "print(decoder_input.shape)\n",
        "print(decoder_target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "provincial-prague",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "provincial-prague",
        "outputId": "16926066-43a9-43cd-e09b-7814c46287dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1932  307   41    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "print(encoder_input[100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "affecting-temperature",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "affecting-temperature",
        "outputId": "b2499591-ad6c-4302-9cda-d908d7310d63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3480\n",
            "7965\n"
          ]
        }
      ],
      "source": [
        "src_vocab_size = len(tok_src.word_index) + 1\n",
        "tar_vocab_size = len(tok_tar.word_index) + 1\n",
        "print(src_vocab_size)\n",
        "print(tar_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "valid-owner",
      "metadata": {
        "id": "valid-owner"
      },
      "outputs": [],
      "source": [
        "# 단어 -> 정수, 정수 -> 단어 딕셔너리 준비\n",
        "src_to_index = tok_src.word_index\n",
        "index_to_src = tok_src.index_word\n",
        "\n",
        "tar_to_index = tok_tar.word_index\n",
        "index_to_tar = tok_tar.index_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "interesting-pressure",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "interesting-pressure",
        "outputId": "81e46342-10c8-447e-a120-d58da088b797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 488 4207  533 ... 5510 2191 2671]\n"
          ]
        }
      ],
      "source": [
        "# 데이터 셔플링 준비: 샘플 id 셔플\n",
        "idx = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(idx)\n",
        "print(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "twelve-saint",
      "metadata": {
        "id": "twelve-saint"
      },
      "outputs": [],
      "source": [
        "# 셔플된 샘플 id 순서대로 재배치\n",
        "encoder_input = encoder_input[idx]\n",
        "decoder_input = decoder_input[idx]\n",
        "decoder_target = decoder_target[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "continental-species",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "continental-species",
        "outputId": "94b74735-58b6-46d9-9add-e06027328524"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2,   16, 1113,    6,    1,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "encoder_input[100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "about-greeting",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "about-greeting",
        "outputId": "7cce7e22-1081-4115-94cb-4a887565cf37"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2,    9,   33, 1070,    7,    1,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "decoder_input[100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thick-devices",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thick-devices",
        "outputId": "ba06cd38-2fd7-4508-8336-e437f808e254"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   9,   33, 1070,    7,    1,    3,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "decoder_target[100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "suitable-junction",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suitable-junction",
        "outputId": "0b3c03e8-60f2-4b37-d224-0193e1dea69a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "589"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# 10%를 검증 데이터로 사용\n",
        "num_of_val = int(encoder_input.shape[0]*0.1)\n",
        "num_of_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "revolutionary-manchester",
      "metadata": {
        "id": "revolutionary-manchester"
      },
      "outputs": [],
      "source": [
        "# 학습 집합과 검증 집합 분리\n",
        "encoder_input_train = encoder_input[:-num_of_val]\n",
        "decoder_input_train = decoder_input[:-num_of_val]\n",
        "decoder_target_train = decoder_target[:-num_of_val]\n",
        "\n",
        "encoder_input_val = encoder_input[-num_of_val:]\n",
        "decoder_input_val = decoder_input[-num_of_val:]\n",
        "decoder_target_val = decoder_target[-num_of_val:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shared-blind",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shared-blind",
        "outputId": "780b5e69-81d2-4785-cbef-22849c88b33a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5301, 110) int32\n",
            "(5301, 96) int32\n",
            "(5301, 96) int32\n",
            "(589, 110) int32\n",
            "(589, 96) int32\n",
            "(589, 96) int32\n"
          ]
        }
      ],
      "source": [
        "print(encoder_input_train.shape, encoder_input.dtype)\n",
        "print(decoder_input_train.shape, decoder_input_train.dtype)\n",
        "print(decoder_target_train.shape, decoder_target_train.dtype)\n",
        "print(encoder_input_val.shape, encoder_input_val.dtype)\n",
        "print(decoder_input_val.shape, decoder_input_val.dtype)\n",
        "print(decoder_target_val.shape, decoder_target_val.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2813b3f",
      "metadata": {
        "id": "e2813b3f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "proof-petite",
      "metadata": {
        "id": "proof-petite"
      },
      "source": [
        "## 기계번역 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tropical-stake",
      "metadata": {
        "id": "tropical-stake"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "basic-nurse",
      "metadata": {
        "id": "basic-nurse"
      },
      "outputs": [],
      "source": [
        "# 임베딩 벡터 크기 및 LSTM 은닉 상태 크기 설정\n",
        "# 실제 문제에 적용할 때는 적절한 값으로 변경 필요\n",
        "embedding_dim = 300\n",
        "hidden_units = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bronze-feelings",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bronze-feelings",
        "outputId": "6de6790c-538e-493d-e866-5e387b52c25a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 200) (None, 200) (None, 200)\n"
          ]
        }
      ],
      "source": [
        "# 인코더 설정\n",
        "encoder_inputs = Input(shape=(None,), dtype='int32', name='enc_input')  # 입력층\n",
        "enc_emb =  Embedding(src_vocab_size, embedding_dim, name='enc_emb')(encoder_inputs)  # 임베딩 층\n",
        "enc_masking = Masking(mask_value=0.0, name='enc_mask')(enc_emb)  # 패딩 마스킹: 패딩 토큰인 0은 연산에서 제외\n",
        "encoder_lstm = LSTM(hidden_units, return_state=True, name='enc_lstm')  # 마지막 상태를 디코더로 전달하기 위해 return_state는 True\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)  # 은닉 상태와 셀 상태를 리턴\n",
        "encoder_states = [state_h, state_c]  # 인코더의 은닉 상태와 셀 상태를 저장\n",
        "print(encoder_outputs.shape, state_h.shape, state_c.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "reported-chester",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reported-chester",
        "outputId": "4650a0ca-6528-4e68-a1b7-1879b29369fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, None, 7965)\n"
          ]
        }
      ],
      "source": [
        "# 디코더 설정\n",
        "decoder_inputs = Input(shape=(None,), dtype='int32', name='dec_input')  # 입력층\n",
        "dec_emb_layer = Embedding(tar_vocab_size, embedding_dim, name='dec_emb')  # 임베딩 층\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "dec_masking = Masking(mask_value=0.0, name='dec_mask')(dec_emb)  # 패딩 마스킹: 패딩 토큰인 0은 연산에서 제외\n",
        "\n",
        "# 상태값 리턴을 위해 return_state는 True, 모든 타임스텝에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
        "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True, name='dec_lstm')\n",
        "\n",
        "# 인코더의 마지막 상태를 디코더의 초기 상태로 사용\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_masking, initial_state=encoder_states)\n",
        "\n",
        "# 모든 타임스텝의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
        "decoder_dense = Dense(tar_vocab_size, activation='softmax', name='dec_dense')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "print(decoder_outputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "worldwide-procurement",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "worldwide-procurement",
        "outputId": "f1e8de62-ce0b-4b5a-8841-4a9f3775ea1d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ enc_input (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ enc_emb (\u001b[38;5;33mEmbedding\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)      │      \u001b[38;5;34m1,044,000\u001b[0m │ enc_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_input (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ enc_emb[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_emb (\u001b[38;5;33mEmbedding\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)      │      \u001b[38;5;34m2,389,500\u001b[0m │ dec_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ enc_mask (\u001b[38;5;33mMasking\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ enc_emb[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ any (\u001b[38;5;33mAny\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_mask (\u001b[38;5;33mMasking\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ dec_emb[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ enc_lstm (\u001b[38;5;33mLSTM\u001b[0m)           │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │        \u001b[38;5;34m400,800\u001b[0m │ enc_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │ \u001b[38;5;34m200\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)]     │                │ any[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_lstm (\u001b[38;5;33mLSTM\u001b[0m)           │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m),    │        \u001b[38;5;34m400,800\u001b[0m │ dec_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │ enc_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],        │\n",
              "│                           │ \u001b[38;5;34m200\u001b[0m)]                  │                │ enc_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_dense (\u001b[38;5;33mDense\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7965\u001b[0m)     │      \u001b[38;5;34m1,600,965\u001b[0m │ dec_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ enc_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ enc_emb (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,044,000</span> │ enc_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ enc_emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_emb (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,389,500</span> │ dec_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ enc_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ enc_emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ any (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Any</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dec_emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ enc_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)           │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">400,800</span> │ enc_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)]     │                │ any[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)           │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>),    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">400,800</span> │ dec_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │ enc_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],        │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)]                  │                │ enc_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7965</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600,965</span> │ dec_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,836,065\u001b[0m (22.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,836,065</span> (22.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,836,065\u001b[0m (22.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,836,065</span> (22.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 학습 모델 정의\n",
        "# 학습 시에는 디코더 입력으로 정답(decoder_inputs)을 사용함 (teacher forcing)\n",
        "# 입력: [인코더 입력, 디코더 입력]\n",
        "# 출력: 디코더 출력\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# 다중 클래스 분류 문제.\n",
        "# 레이블이 원-핫 인코딩 되지 않은 정수값이므로, 손실 함수는 sparse_categorical_crossentropy\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "resistant-sunset",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "resistant-sunset",
        "outputId": "39af60df-ec7a-4c26-d83e-0106e9f23c45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 84ms/step - acc: 0.8803 - loss: 3.6395 - val_acc: 0.9453 - val_loss: 0.3953\n",
            "Epoch 2/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 78ms/step - acc: 0.9471 - loss: 0.3916 - val_acc: 0.9565 - val_loss: 0.3693\n",
            "Epoch 3/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - acc: 0.9553 - loss: 0.3669 - val_acc: 0.9570 - val_loss: 0.3583\n",
            "Epoch 4/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 81ms/step - acc: 0.9553 - loss: 0.3561 - val_acc: 0.9570 - val_loss: 0.3538\n",
            "Epoch 5/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - acc: 0.9556 - loss: 0.3451 - val_acc: 0.9570 - val_loss: 0.3511\n",
            "Epoch 6/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - acc: 0.9553 - loss: 0.3413 - val_acc: 0.9570 - val_loss: 0.3494\n",
            "Epoch 7/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 80ms/step - acc: 0.9559 - loss: 0.3321 - val_acc: 0.9571 - val_loss: 0.3494\n",
            "Epoch 8/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 84ms/step - acc: 0.9558 - loss: 0.3270 - val_acc: 0.9572 - val_loss: 0.3486\n",
            "Epoch 9/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 81ms/step - acc: 0.9556 - loss: 0.3243 - val_acc: 0.9573 - val_loss: 0.3488\n",
            "Epoch 10/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 85ms/step - acc: 0.9556 - loss: 0.3217 - val_acc: 0.9573 - val_loss: 0.3496\n",
            "Epoch 11/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - acc: 0.9557 - loss: 0.3183 - val_acc: 0.9562 - val_loss: 0.3506\n",
            "Epoch 12/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - acc: 0.9557 - loss: 0.3146 - val_acc: 0.9558 - val_loss: 0.3516\n",
            "Epoch 13/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - acc: 0.9573 - loss: 0.3034 - val_acc: 0.9556 - val_loss: 0.3527\n",
            "Epoch 14/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - acc: 0.9572 - loss: 0.3027 - val_acc: 0.9558 - val_loss: 0.3542\n",
            "Epoch 15/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9575 - loss: 0.2985 - val_acc: 0.9561 - val_loss: 0.3546\n",
            "Epoch 16/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9575 - loss: 0.2962 - val_acc: 0.9560 - val_loss: 0.3556\n",
            "Epoch 17/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - acc: 0.9582 - loss: 0.2899 - val_acc: 0.9564 - val_loss: 0.3532\n",
            "Epoch 18/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - acc: 0.9587 - loss: 0.2845 - val_acc: 0.9565 - val_loss: 0.3565\n",
            "Epoch 19/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - acc: 0.9580 - loss: 0.2840 - val_acc: 0.9566 - val_loss: 0.3585\n",
            "Epoch 20/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - acc: 0.9594 - loss: 0.2734 - val_acc: 0.9565 - val_loss: 0.3597\n",
            "Epoch 21/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - acc: 0.9598 - loss: 0.2662 - val_acc: 0.9566 - val_loss: 0.3612\n",
            "Epoch 22/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9598 - loss: 0.2639 - val_acc: 0.9569 - val_loss: 0.3606\n",
            "Epoch 23/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - acc: 0.9603 - loss: 0.2584 - val_acc: 0.9570 - val_loss: 0.3623\n",
            "Epoch 24/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - acc: 0.9612 - loss: 0.2495 - val_acc: 0.9570 - val_loss: 0.3628\n",
            "Epoch 25/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9605 - loss: 0.2499 - val_acc: 0.9572 - val_loss: 0.3629\n",
            "Epoch 26/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - acc: 0.9617 - loss: 0.2396 - val_acc: 0.9571 - val_loss: 0.3641\n",
            "Epoch 27/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - acc: 0.9616 - loss: 0.2387 - val_acc: 0.9576 - val_loss: 0.3641\n",
            "Epoch 28/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - acc: 0.9629 - loss: 0.2296 - val_acc: 0.9578 - val_loss: 0.3631\n",
            "Epoch 29/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - acc: 0.9639 - loss: 0.2210 - val_acc: 0.9582 - val_loss: 0.3630\n",
            "Epoch 30/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - acc: 0.9649 - loss: 0.2133 - val_acc: 0.9584 - val_loss: 0.3636\n",
            "Epoch 31/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - acc: 0.9653 - loss: 0.2078 - val_acc: 0.9585 - val_loss: 0.3646\n",
            "Epoch 32/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9661 - loss: 0.2019 - val_acc: 0.9585 - val_loss: 0.3653\n",
            "Epoch 33/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9667 - loss: 0.1969 - val_acc: 0.9587 - val_loss: 0.3666\n",
            "Epoch 34/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9681 - loss: 0.1884 - val_acc: 0.9587 - val_loss: 0.3680\n",
            "Epoch 35/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9683 - loss: 0.1855 - val_acc: 0.9591 - val_loss: 0.3674\n",
            "Epoch 36/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9697 - loss: 0.1777 - val_acc: 0.9592 - val_loss: 0.3695\n",
            "Epoch 37/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - acc: 0.9715 - loss: 0.1698 - val_acc: 0.9594 - val_loss: 0.3698\n",
            "Epoch 38/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - acc: 0.9721 - loss: 0.1661 - val_acc: 0.9595 - val_loss: 0.3707\n",
            "Epoch 39/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - acc: 0.9736 - loss: 0.1592 - val_acc: 0.9596 - val_loss: 0.3720\n",
            "Epoch 40/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - acc: 0.9748 - loss: 0.1524 - val_acc: 0.9601 - val_loss: 0.3724\n",
            "Epoch 41/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - acc: 0.9760 - loss: 0.1477 - val_acc: 0.9600 - val_loss: 0.3738\n",
            "Epoch 42/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - acc: 0.9773 - loss: 0.1416 - val_acc: 0.9603 - val_loss: 0.3735\n",
            "Epoch 43/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9781 - loss: 0.1367 - val_acc: 0.9603 - val_loss: 0.3753\n",
            "Epoch 44/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - acc: 0.9787 - loss: 0.1317 - val_acc: 0.9605 - val_loss: 0.3762\n",
            "Epoch 45/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9796 - loss: 0.1260 - val_acc: 0.9604 - val_loss: 0.3776\n",
            "Epoch 46/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9803 - loss: 0.1214 - val_acc: 0.9605 - val_loss: 0.3783\n",
            "Epoch 47/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9809 - loss: 0.1177 - val_acc: 0.9607 - val_loss: 0.3785\n",
            "Epoch 48/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - acc: 0.9816 - loss: 0.1121 - val_acc: 0.9606 - val_loss: 0.3813\n",
            "Epoch 49/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - acc: 0.9819 - loss: 0.1090 - val_acc: 0.9607 - val_loss: 0.3814\n",
            "Epoch 50/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - acc: 0.9825 - loss: 0.1050 - val_acc: 0.9607 - val_loss: 0.3824\n",
            "Epoch 51/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step - acc: 0.9827 - loss: 0.1019 - val_acc: 0.9609 - val_loss: 0.3836\n",
            "Epoch 52/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - acc: 0.9831 - loss: 0.0997 - val_acc: 0.9609 - val_loss: 0.3842\n",
            "Epoch 53/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - acc: 0.9835 - loss: 0.0956 - val_acc: 0.9608 - val_loss: 0.3862\n",
            "Epoch 54/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9839 - loss: 0.0933 - val_acc: 0.9610 - val_loss: 0.3868\n",
            "Epoch 55/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9840 - loss: 0.0914 - val_acc: 0.9610 - val_loss: 0.3880\n",
            "Epoch 56/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - acc: 0.9842 - loss: 0.0893 - val_acc: 0.9610 - val_loss: 0.3894\n",
            "Epoch 57/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9845 - loss: 0.0862 - val_acc: 0.9610 - val_loss: 0.3896\n",
            "Epoch 58/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9849 - loss: 0.0842 - val_acc: 0.9611 - val_loss: 0.3914\n",
            "Epoch 59/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - acc: 0.9850 - loss: 0.0823 - val_acc: 0.9609 - val_loss: 0.3926\n",
            "Epoch 60/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9852 - loss: 0.0803 - val_acc: 0.9612 - val_loss: 0.3934\n",
            "Epoch 61/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - acc: 0.9856 - loss: 0.0779 - val_acc: 0.9613 - val_loss: 0.3941\n",
            "Epoch 62/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9857 - loss: 0.0764 - val_acc: 0.9610 - val_loss: 0.3953\n",
            "Epoch 63/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - acc: 0.9861 - loss: 0.0748 - val_acc: 0.9614 - val_loss: 0.3956\n",
            "Epoch 64/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - acc: 0.9861 - loss: 0.0727 - val_acc: 0.9612 - val_loss: 0.3974\n",
            "Epoch 65/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - acc: 0.9863 - loss: 0.0709 - val_acc: 0.9614 - val_loss: 0.3984\n",
            "Epoch 66/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - acc: 0.9864 - loss: 0.0697 - val_acc: 0.9613 - val_loss: 0.3981\n",
            "Epoch 67/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9867 - loss: 0.0684 - val_acc: 0.9616 - val_loss: 0.3999\n",
            "Epoch 68/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9869 - loss: 0.0668 - val_acc: 0.9614 - val_loss: 0.4012\n",
            "Epoch 69/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9871 - loss: 0.0653 - val_acc: 0.9615 - val_loss: 0.4017\n",
            "Epoch 70/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9872 - loss: 0.0645 - val_acc: 0.9611 - val_loss: 0.4037\n",
            "Epoch 71/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9872 - loss: 0.0632 - val_acc: 0.9616 - val_loss: 0.4041\n",
            "Epoch 72/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9875 - loss: 0.0618 - val_acc: 0.9616 - val_loss: 0.4051\n",
            "Epoch 73/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9876 - loss: 0.0606 - val_acc: 0.9613 - val_loss: 0.4068\n",
            "Epoch 74/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9877 - loss: 0.0601 - val_acc: 0.9613 - val_loss: 0.4070\n",
            "Epoch 75/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - acc: 0.9879 - loss: 0.0582 - val_acc: 0.9615 - val_loss: 0.4091\n",
            "Epoch 76/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9881 - loss: 0.0570 - val_acc: 0.9614 - val_loss: 0.4086\n",
            "Epoch 77/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9880 - loss: 0.0565 - val_acc: 0.9614 - val_loss: 0.4112\n",
            "Epoch 78/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - acc: 0.9883 - loss: 0.0553 - val_acc: 0.9616 - val_loss: 0.4109\n",
            "Epoch 79/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - acc: 0.9884 - loss: 0.0543 - val_acc: 0.9614 - val_loss: 0.4121\n",
            "Epoch 80/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9886 - loss: 0.0533 - val_acc: 0.9613 - val_loss: 0.4134\n",
            "Epoch 81/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9888 - loss: 0.0524 - val_acc: 0.9614 - val_loss: 0.4137\n",
            "Epoch 82/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - acc: 0.9888 - loss: 0.0515 - val_acc: 0.9612 - val_loss: 0.4147\n",
            "Epoch 83/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - acc: 0.9890 - loss: 0.0509 - val_acc: 0.9614 - val_loss: 0.4156\n",
            "Epoch 84/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - acc: 0.9891 - loss: 0.0498 - val_acc: 0.9613 - val_loss: 0.4170\n",
            "Epoch 85/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - acc: 0.9893 - loss: 0.0488 - val_acc: 0.9614 - val_loss: 0.4170\n",
            "Epoch 86/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9896 - loss: 0.0477 - val_acc: 0.9614 - val_loss: 0.4184\n",
            "Epoch 87/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9896 - loss: 0.0467 - val_acc: 0.9615 - val_loss: 0.4196\n",
            "Epoch 88/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9898 - loss: 0.0461 - val_acc: 0.9615 - val_loss: 0.4192\n",
            "Epoch 89/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9899 - loss: 0.0450 - val_acc: 0.9614 - val_loss: 0.4215\n",
            "Epoch 90/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9901 - loss: 0.0442 - val_acc: 0.9613 - val_loss: 0.4216\n",
            "Epoch 91/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9901 - loss: 0.0436 - val_acc: 0.9614 - val_loss: 0.4219\n",
            "Epoch 92/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9902 - loss: 0.0429 - val_acc: 0.9613 - val_loss: 0.4237\n",
            "Epoch 93/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - acc: 0.9906 - loss: 0.0416 - val_acc: 0.9616 - val_loss: 0.4237\n",
            "Epoch 94/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - acc: 0.9906 - loss: 0.0409 - val_acc: 0.9617 - val_loss: 0.4246\n",
            "Epoch 95/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - acc: 0.9908 - loss: 0.0409 - val_acc: 0.9613 - val_loss: 0.4259\n",
            "Epoch 96/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9909 - loss: 0.0394 - val_acc: 0.9615 - val_loss: 0.4269\n",
            "Epoch 97/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9912 - loss: 0.0391 - val_acc: 0.9618 - val_loss: 0.4266\n",
            "Epoch 98/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - acc: 0.9912 - loss: 0.0380 - val_acc: 0.9618 - val_loss: 0.4284\n",
            "Epoch 99/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9917 - loss: 0.0372 - val_acc: 0.9617 - val_loss: 0.4288\n",
            "Epoch 100/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9915 - loss: 0.0367 - val_acc: 0.9617 - val_loss: 0.4299\n",
            "Epoch 101/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9917 - loss: 0.0361 - val_acc: 0.9618 - val_loss: 0.4300\n",
            "Epoch 102/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9919 - loss: 0.0352 - val_acc: 0.9618 - val_loss: 0.4311\n",
            "Epoch 103/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9920 - loss: 0.0348 - val_acc: 0.9618 - val_loss: 0.4312\n",
            "Epoch 104/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9922 - loss: 0.0338 - val_acc: 0.9618 - val_loss: 0.4327\n",
            "Epoch 105/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9926 - loss: 0.0329 - val_acc: 0.9619 - val_loss: 0.4340\n",
            "Epoch 106/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9926 - loss: 0.0321 - val_acc: 0.9618 - val_loss: 0.4349\n",
            "Epoch 107/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9927 - loss: 0.0317 - val_acc: 0.9619 - val_loss: 0.4354\n",
            "Epoch 108/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - acc: 0.9929 - loss: 0.0310 - val_acc: 0.9619 - val_loss: 0.4351\n",
            "Epoch 109/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - acc: 0.9929 - loss: 0.0306 - val_acc: 0.9618 - val_loss: 0.4366\n",
            "Epoch 110/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9931 - loss: 0.0295 - val_acc: 0.9620 - val_loss: 0.4381\n",
            "Epoch 111/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9934 - loss: 0.0289 - val_acc: 0.9619 - val_loss: 0.4382\n",
            "Epoch 112/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9936 - loss: 0.0282 - val_acc: 0.9619 - val_loss: 0.4383\n",
            "Epoch 113/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9937 - loss: 0.0279 - val_acc: 0.9620 - val_loss: 0.4392\n",
            "Epoch 114/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9939 - loss: 0.0269 - val_acc: 0.9620 - val_loss: 0.4401\n",
            "Epoch 115/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - acc: 0.9941 - loss: 0.0264 - val_acc: 0.9621 - val_loss: 0.4406\n",
            "Epoch 116/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - acc: 0.9942 - loss: 0.0257 - val_acc: 0.9618 - val_loss: 0.4425\n",
            "Epoch 117/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - acc: 0.9943 - loss: 0.0254 - val_acc: 0.9620 - val_loss: 0.4433\n",
            "Epoch 118/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9945 - loss: 0.0247 - val_acc: 0.9620 - val_loss: 0.4436\n",
            "Epoch 119/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9947 - loss: 0.0241 - val_acc: 0.9619 - val_loss: 0.4446\n",
            "Epoch 120/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - acc: 0.9949 - loss: 0.0234 - val_acc: 0.9619 - val_loss: 0.4475\n",
            "Epoch 121/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9948 - loss: 0.0231 - val_acc: 0.9620 - val_loss: 0.4463\n",
            "Epoch 122/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9949 - loss: 0.0226 - val_acc: 0.9620 - val_loss: 0.4471\n",
            "Epoch 123/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9950 - loss: 0.0220 - val_acc: 0.9621 - val_loss: 0.4478\n",
            "Epoch 124/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - acc: 0.9955 - loss: 0.0210 - val_acc: 0.9619 - val_loss: 0.4484\n",
            "Epoch 125/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - acc: 0.9954 - loss: 0.0208 - val_acc: 0.9620 - val_loss: 0.4495\n",
            "Epoch 126/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - acc: 0.9959 - loss: 0.0196 - val_acc: 0.9619 - val_loss: 0.4509\n",
            "Epoch 127/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - acc: 0.9957 - loss: 0.0195 - val_acc: 0.9619 - val_loss: 0.4518\n",
            "Epoch 128/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9959 - loss: 0.0193 - val_acc: 0.9621 - val_loss: 0.4518\n",
            "Epoch 129/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9960 - loss: 0.0186 - val_acc: 0.9620 - val_loss: 0.4532\n",
            "Epoch 130/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - acc: 0.9961 - loss: 0.0184 - val_acc: 0.9621 - val_loss: 0.4530\n",
            "Epoch 131/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - acc: 0.9962 - loss: 0.0179 - val_acc: 0.9619 - val_loss: 0.4546\n",
            "Epoch 132/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9962 - loss: 0.0175 - val_acc: 0.9621 - val_loss: 0.4541\n",
            "Epoch 133/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - acc: 0.9964 - loss: 0.0169 - val_acc: 0.9619 - val_loss: 0.4560\n",
            "Epoch 134/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - acc: 0.9965 - loss: 0.0163 - val_acc: 0.9618 - val_loss: 0.4565\n",
            "Epoch 135/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9965 - loss: 0.0162 - val_acc: 0.9620 - val_loss: 0.4569\n",
            "Epoch 136/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9965 - loss: 0.0161 - val_acc: 0.9620 - val_loss: 0.4584\n",
            "Epoch 137/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9969 - loss: 0.0149 - val_acc: 0.9622 - val_loss: 0.4594\n",
            "Epoch 138/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9967 - loss: 0.0150 - val_acc: 0.9619 - val_loss: 0.4595\n",
            "Epoch 139/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9969 - loss: 0.0146 - val_acc: 0.9619 - val_loss: 0.4623\n",
            "Epoch 140/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9970 - loss: 0.0141 - val_acc: 0.9621 - val_loss: 0.4615\n",
            "Epoch 141/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 88ms/step - acc: 0.9972 - loss: 0.0134 - val_acc: 0.9622 - val_loss: 0.4626\n",
            "Epoch 142/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - acc: 0.9972 - loss: 0.0133 - val_acc: 0.9621 - val_loss: 0.4619\n",
            "Epoch 143/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - acc: 0.9973 - loss: 0.0129 - val_acc: 0.9621 - val_loss: 0.4633\n",
            "Epoch 144/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9975 - loss: 0.0123 - val_acc: 0.9620 - val_loss: 0.4646\n",
            "Epoch 145/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - acc: 0.9974 - loss: 0.0121 - val_acc: 0.9621 - val_loss: 0.4654\n",
            "Epoch 146/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9975 - loss: 0.0117 - val_acc: 0.9621 - val_loss: 0.4661\n",
            "Epoch 147/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9976 - loss: 0.0113 - val_acc: 0.9621 - val_loss: 0.4665\n",
            "Epoch 148/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9977 - loss: 0.0110 - val_acc: 0.9621 - val_loss: 0.4674\n",
            "Epoch 149/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - acc: 0.9976 - loss: 0.0110 - val_acc: 0.9620 - val_loss: 0.4688\n",
            "Epoch 150/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9977 - loss: 0.0106 - val_acc: 0.9621 - val_loss: 0.4704\n",
            "Epoch 151/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9977 - loss: 0.0105 - val_acc: 0.9618 - val_loss: 0.4712\n",
            "Epoch 152/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - acc: 0.9978 - loss: 0.0101 - val_acc: 0.9622 - val_loss: 0.4712\n",
            "Epoch 153/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - acc: 0.9979 - loss: 0.0097 - val_acc: 0.9622 - val_loss: 0.4719\n",
            "Epoch 154/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - acc: 0.9979 - loss: 0.0094 - val_acc: 0.9620 - val_loss: 0.4727\n",
            "Epoch 155/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9979 - loss: 0.0091 - val_acc: 0.9622 - val_loss: 0.4725\n",
            "Epoch 156/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 88ms/step - acc: 0.9980 - loss: 0.0087 - val_acc: 0.9620 - val_loss: 0.4745\n",
            "Epoch 157/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - acc: 0.9980 - loss: 0.0083 - val_acc: 0.9622 - val_loss: 0.4742\n",
            "Epoch 158/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - acc: 0.9979 - loss: 0.0086 - val_acc: 0.9620 - val_loss: 0.4759\n",
            "Epoch 159/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9982 - loss: 0.0080 - val_acc: 0.9622 - val_loss: 0.4763\n",
            "Epoch 160/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9981 - loss: 0.0081 - val_acc: 0.9620 - val_loss: 0.4760\n",
            "Epoch 161/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9982 - loss: 0.0078 - val_acc: 0.9621 - val_loss: 0.4769\n",
            "Epoch 162/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - acc: 0.9983 - loss: 0.0073 - val_acc: 0.9619 - val_loss: 0.4775\n",
            "Epoch 163/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 88ms/step - acc: 0.9982 - loss: 0.0072 - val_acc: 0.9622 - val_loss: 0.4778\n",
            "Epoch 164/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9984 - loss: 0.0071 - val_acc: 0.9622 - val_loss: 0.4799\n",
            "Epoch 165/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9983 - loss: 0.0070 - val_acc: 0.9622 - val_loss: 0.4799\n",
            "Epoch 166/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - acc: 0.9983 - loss: 0.0068 - val_acc: 0.9623 - val_loss: 0.4803\n",
            "Epoch 167/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 88ms/step - acc: 0.9984 - loss: 0.0066 - val_acc: 0.9621 - val_loss: 0.4820\n",
            "Epoch 168/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - acc: 0.9984 - loss: 0.0064 - val_acc: 0.9621 - val_loss: 0.4831\n",
            "Epoch 169/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - acc: 0.9984 - loss: 0.0065 - val_acc: 0.9622 - val_loss: 0.4813\n",
            "Epoch 170/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - acc: 0.9985 - loss: 0.0063 - val_acc: 0.9622 - val_loss: 0.4830\n",
            "Epoch 171/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - acc: 0.9985 - loss: 0.0058 - val_acc: 0.9622 - val_loss: 0.4851\n",
            "Epoch 172/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 88ms/step - acc: 0.9984 - loss: 0.0058 - val_acc: 0.9620 - val_loss: 0.4852\n",
            "Epoch 173/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 89ms/step - acc: 0.9985 - loss: 0.0057 - val_acc: 0.9622 - val_loss: 0.4845\n",
            "Epoch 174/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - acc: 0.9985 - loss: 0.0057 - val_acc: 0.9622 - val_loss: 0.4847\n",
            "Epoch 175/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9985 - loss: 0.0053 - val_acc: 0.9620 - val_loss: 0.4860\n",
            "Epoch 176/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 86ms/step - acc: 0.9987 - loss: 0.0052 - val_acc: 0.9620 - val_loss: 0.4873\n",
            "Epoch 177/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - acc: 0.9985 - loss: 0.0051 - val_acc: 0.9620 - val_loss: 0.4873\n",
            "Epoch 178/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - acc: 0.9986 - loss: 0.0047 - val_acc: 0.9619 - val_loss: 0.4879\n",
            "Epoch 179/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9987 - loss: 0.0047 - val_acc: 0.9621 - val_loss: 0.4884\n",
            "Epoch 180/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - acc: 0.9985 - loss: 0.0046 - val_acc: 0.9620 - val_loss: 0.4903\n",
            "Epoch 181/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9985 - loss: 0.0047 - val_acc: 0.9621 - val_loss: 0.4888\n",
            "Epoch 182/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9986 - loss: 0.0046 - val_acc: 0.9621 - val_loss: 0.4907\n",
            "Epoch 183/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 84ms/step - acc: 0.9986 - loss: 0.0044 - val_acc: 0.9621 - val_loss: 0.4900\n",
            "Epoch 184/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 87ms/step - acc: 0.9985 - loss: 0.0046 - val_acc: 0.9620 - val_loss: 0.4926\n",
            "Epoch 185/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 88ms/step - acc: 0.9985 - loss: 0.0047 - val_acc: 0.9621 - val_loss: 0.4926\n",
            "Epoch 186/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - acc: 0.9985 - loss: 0.0048 - val_acc: 0.9618 - val_loss: 0.4946\n",
            "Epoch 187/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 88ms/step - acc: 0.9984 - loss: 0.0052 - val_acc: 0.9620 - val_loss: 0.4954\n",
            "Epoch 188/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - acc: 0.9986 - loss: 0.0046 - val_acc: 0.9620 - val_loss: 0.4943\n",
            "Epoch 189/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9985 - loss: 0.0049 - val_acc: 0.9619 - val_loss: 0.4975\n",
            "Epoch 190/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9984 - loss: 0.0047 - val_acc: 0.9622 - val_loss: 0.4984\n",
            "Epoch 191/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - acc: 0.9985 - loss: 0.0047 - val_acc: 0.9622 - val_loss: 0.4967\n",
            "Epoch 192/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - acc: 0.9985 - loss: 0.0047 - val_acc: 0.9620 - val_loss: 0.4965\n",
            "Epoch 193/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 88ms/step - acc: 0.9986 - loss: 0.0042 - val_acc: 0.9620 - val_loss: 0.4969\n",
            "Epoch 194/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - acc: 0.9986 - loss: 0.0038 - val_acc: 0.9622 - val_loss: 0.4972\n",
            "Epoch 195/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - acc: 0.9985 - loss: 0.0039 - val_acc: 0.9622 - val_loss: 0.4983\n",
            "Epoch 196/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - acc: 0.9986 - loss: 0.0036 - val_acc: 0.9623 - val_loss: 0.5001\n",
            "Epoch 197/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 85ms/step - acc: 0.9987 - loss: 0.0034 - val_acc: 0.9620 - val_loss: 0.4998\n",
            "Epoch 198/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 88ms/step - acc: 0.9987 - loss: 0.0033 - val_acc: 0.9622 - val_loss: 0.4995\n",
            "Epoch 199/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - acc: 0.9987 - loss: 0.0031 - val_acc: 0.9620 - val_loss: 0.5004\n",
            "Epoch 200/200\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 89ms/step - acc: 0.9986 - loss: 0.0032 - val_acc: 0.9621 - val_loss: 0.5011\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train,\n",
        "              validation_data=([encoder_input_val, decoder_input_val], decoder_target_val),\n",
        "              batch_size = 64, epochs = 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hairy-attribute",
      "metadata": {
        "id": "hairy-attribute"
      },
      "source": [
        "## 기계번역 추론/예측\n",
        "학습 결과 적용하여 번역하기. 학습과 추론/예측 과정이 다르므로 모델을 다시 설계해야 함."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e6d13a4",
      "metadata": {
        "id": "9e6d13a4"
      },
      "outputs": [],
      "source": [
        "# 인코더 모델 정의\n",
        "# 학습 과정에서 사용한 인코더 그대로 재사용\n",
        "encoder_model = Model(encoder_inputs, encoder_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "champion-assets",
      "metadata": {
        "id": "champion-assets"
      },
      "outputs": [],
      "source": [
        "# 디코더 모델 정의\n",
        "# 추론/예측 시에는 디코더 입력으로 이전 타임 스텝의 디코더 출력(상태)을 사용함\n",
        "# 이전 타임 스텝의 상태를 보관할 텐서\n",
        "decoder_state_input_h = Input(shape=(hidden_units,))  # 입력층: 은닉 상태\n",
        "decoder_state_input_c = Input(shape=(hidden_units,))  # 입력층: 셀 상태\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# 학습 때 사용했던 임베딩 층을 재사용\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# 다음 단어 예측을 위해 이전 타임 스텝의 상태를 현 타임 스텝의 초기 상태로 사용\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "# 모든 타임 스텝에 대해서 단어 예측\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# 디코더 모델\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26c8414e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "26c8414e",
        "outputId": "2389d049-5dcd-41c6-d2cb-39dacd441f88"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ dec_input (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_emb (\u001b[38;5;33mEmbedding\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)      │      \u001b[38;5;34m2,389,500\u001b[0m │ dec_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_lstm (\u001b[38;5;33mLSTM\u001b[0m)           │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m),    │        \u001b[38;5;34m400,800\u001b[0m │ dec_emb[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │ \u001b[38;5;34m200\u001b[0m)]                  │                │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_dense (\u001b[38;5;33mDense\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7965\u001b[0m)     │      \u001b[38;5;34m1,600,965\u001b[0m │ dec_lstm[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ dec_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_emb (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,389,500</span> │ dec_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)           │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>),    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">400,800</span> │ dec_emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)]                  │                │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7965</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600,965</span> │ dec_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,391,265\u001b[0m (16.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,391,265</span> (16.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,391,265\u001b[0m (16.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,391,265</span> (16.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "decoder_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "egyptian-shell",
      "metadata": {
        "id": "egyptian-shell"
      },
      "outputs": [],
      "source": [
        "# 번역 실행을 위한 함수\n",
        "def decode_sequence(input_seq, limit=100):\n",
        "    # 입력으로부터 인코더의 상태를 얻음\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # <sos>에 해당하는 정수 생성\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = tok_tar.word_index['<sos>']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    # stop_condition이 True가 될 때까지 루프 반복\n",
        "    while not stop_condition:\n",
        "        # 이전 타임 스텝의 상태 states_value를 현 타임 스텝의 입력으로 사용\n",
        "        # 최소 상태 값은 인코더의 마지막 상태 값\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # 예측 결과를 단어로 변환\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        if sampled_token_index != 0:\n",
        "            sampled_word = tok_tar.index_word[sampled_token_index]\n",
        "        else:\n",
        "            sampled_word = '_'\n",
        "\n",
        "        # 현재 타임 스텝의 예측 단어를 예측 문장에 추가\n",
        "        decoded_sentence += ' '+sampled_word\n",
        "\n",
        "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단\n",
        "        if (sampled_word == '<eos>' or len(decoded_sentence) > limit):\n",
        "            stop_condition = True\n",
        "\n",
        "        # 현재 타임 스텝의 예측 결과를 다음 타임 스텝의 입력으로 사용하기 위해 저장\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # 현재 타임 스텝의 상태를 다음 타임 스텝의 입력으로 사용하기 위해 저장\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beneficial-application",
      "metadata": {
        "id": "beneficial-application"
      },
      "outputs": [],
      "source": [
        "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2src(input_seq):\n",
        "    text = ''\n",
        "    for i in input_seq:\n",
        "        if(i != 0):\n",
        "            text = text + tok_src.index_word[i] + ' '\n",
        "    return text\n",
        "\n",
        "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2tar(input_seq):\n",
        "    text = ''\n",
        "    for i in input_seq:\n",
        "        if((i != 0 and i != tok_tar.word_index['<sos>']) and i != tok_tar.word_index['<eos>']):\n",
        "            text = text + tok_tar.index_word[i] + ' '\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spiritual-uganda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spiritual-uganda",
        "outputId": "9a94696a-cb5b-456b-e7c5-a724be6ced9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "원문:  Keep listening . \n",
            "번역문(정답):   계속 들어 . \n",
            "번역문(시스템):   계속 들어 . \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "원문:  I m behind you . \n",
            "번역문(정답):   난 네 뒤에 있어 . \n",
            "번역문(시스템):   난 네 뒤에 있어 . \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "원문:  He is a rude person . \n",
            "번역문(정답):   그는 버릇이 없다 . \n",
            "번역문(시스템):   그는 버릇이 없다 . \n"
          ]
        }
      ],
      "source": [
        "# 학습 집합 내 샘플 문장에 대해 번역 결과 확인\n",
        "sample_idx = [10, 100, 1000]\n",
        "for i in sample_idx:\n",
        "    input_seq = encoder_input_train[i: i+1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "    print('원문: ', seq2src(encoder_input_train[i]))\n",
        "    print('번역문(정답):  ', seq2tar(decoder_input_train[i]))\n",
        "    print('번역문(시스템): ', decoded_sentence[:-5])  # 마지막 부분의 '<eos>'는 제외하고 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thorough-discrimination",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thorough-discrimination",
        "outputId": "cb29f46f-50f3-4687-c868-d867e3cde963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "원문:  Tom is smiling . \n",
            "번역문(정답):   톰은 웃고 있다 . \n",
            "번역문(시스템):   톰은 미소를 짓고 있다 . \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "원문:  Tom is multilingual . \n",
            "번역문(정답):   톰은 다국어 사용자야 . \n",
            "번역문(시스템):   톰은 상당히 졸려 보인다 . \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "원문:  Do you like fish ? \n",
            "번역문(정답):   물고기 좋아해 ? \n",
            "번역문(시스템):   생선 좋아해요 ? \n"
          ]
        }
      ],
      "source": [
        "# 검증 집합 내 샘플 문장에 대해 번역 결과 확인\n",
        "sample_idx = [100, 200, 300]\n",
        "for i in sample_idx:\n",
        "    input_seq = encoder_input_val[i: i+1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "    print('원문: ', seq2src(encoder_input_val[i]))\n",
        "    print('번역문(정답):  ', seq2tar(decoder_input_val[i]))\n",
        "    print('번역문(시스템): ', decoded_sentence[:-5])  # 마지막 부분의 '<eos>'는 제외하고 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e01c04ea",
      "metadata": {
        "id": "e01c04ea"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}